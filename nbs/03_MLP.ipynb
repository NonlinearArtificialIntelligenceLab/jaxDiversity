{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: XLA_PYTHON_CLIENT_ALLOCATOR=platform\n"
     ]
    }
   ],
   "source": [
    "%env XLA_PYTHON_CLIENT_ALLOCATOR=platform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP\n",
    "\n",
    "> module for MLP that takes in multiple activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "from fastcore.test import test_eq\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import equinox as eqx\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def trunc_init(weight: jax.Array, key: jax.random.PRNGKey) -> jax.Array:\n",
    "    \"\"\"truncated normal initialization\"\"\"\n",
    "    out, in_ = weight.shape\n",
    "    stddev = jnp.sqrt(1 / in_)\n",
    "    return stddev * jax.random.truncated_normal(key, lower=-2, upper=2)\n",
    "\n",
    "\n",
    "def deterministic_init(weight: jax.Array, key: jax.random.PRNGKey) -> jax.Array:\n",
    "    \"\"\"constant initialization\n",
    "    parameters only for consistency with other initializations\"\"\"\n",
    "    return jnp.ones(weight.shape) * 1e-3\n",
    "\n",
    "\n",
    "def xavier_normal_init(weight: jax.Array, key: jax.random.PRNGKey) -> jax.Array:\n",
    "    \"\"\"xavier normal initialization\"\"\"\n",
    "    out, in_ = weight.shape\n",
    "    stddev = jnp.sqrt(2 / in_)\n",
    "    return stddev * jax.random.normal(key, shape=weight.shape)\n",
    "\n",
    "\n",
    "def xavier_uniform_init(weight: jax.Array, key: jax.random.PRNGKey) -> jax.Array:\n",
    "    \"\"\"xavier uniform initialization\"\"\"\n",
    "    out, in_ = weight.shape\n",
    "    bound = jnp.sqrt(6 / in_)\n",
    "    return bound * jax.random.uniform(key, shape=weight.shape, minval=-1, maxval=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def init_linear_weight(model, init_fn, key):\n",
    "    \"\"\"initialize linear weights of a model with a given init_fn\"\"\"\n",
    "\n",
    "    def is_linear(x):\n",
    "        return isinstance(x, eqx.nn.Linear)\n",
    "\n",
    "    def get_weights(m):\n",
    "        return [\n",
    "            x.weight\n",
    "            for x in jax.tree_util.tree_leaves(m, is_leaf=is_linear)\n",
    "            if is_linear(x)\n",
    "        ]\n",
    "\n",
    "    weights = get_weights(model)\n",
    "    new_weights = [\n",
    "        init_fn(weight, subkey)\n",
    "        for weight, subkey in zip(weights, jax.random.split(key, len(weights)))\n",
    "    ]\n",
    "    new_model = eqx.tree_at(get_weights, model, new_weights)\n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class MultiActMLP(eqx.Module):\n",
    "    input_dim: int\n",
    "    output_dim: int\n",
    "    hidden_layer_sizes: list\n",
    "    layers: list\n",
    "    bias: bool  # whether to include bias in the linear layers\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, hidden_layer_sizes, key, bias=True):\n",
    "        super().__init__()\n",
    "        self.bias = bias\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_layer_sizes = [input_dim] + hidden_layer_sizes + [output_dim]\n",
    "        self.layers = []\n",
    "        keys = jax.random.split(key, len(self.hidden_layer_sizes))\n",
    "        for i in range(len(self.hidden_layer_sizes) - 1):\n",
    "            layer = eqx.nn.Linear(\n",
    "                self.hidden_layer_sizes[i],\n",
    "                self.hidden_layer_sizes[i + 1],\n",
    "                key=keys[i],\n",
    "                use_bias=bias,\n",
    "            )\n",
    "            self.layers.append(layer)\n",
    "\n",
    "    def __call__(self, x, afuncs):\n",
    "        \"\"\"\n",
    "        x: input data\n",
    "        afuncs: activation functions\n",
    "        splits the layers into sections and applies the activation function\n",
    "        \"\"\"\n",
    "        activity = []\n",
    "        for layer in self.layers[:-1]:\n",
    "            split_idx = int(layer.weight.shape[0] / len(afuncs))\n",
    "            x = layer(x)\n",
    "            activity.append(x)\n",
    "            for i, afunc in enumerate(afuncs):\n",
    "                # applies activation functions to each split of layer\n",
    "                x = x.at[i * split_idx : (i + 1) * split_idx].set(\n",
    "                    afunc(x[i * split_idx : (i + 1) * split_idx])\n",
    "                )\n",
    "\n",
    "        # return self.apply_linear(x, self.layers[-1]), activity\n",
    "        return self.layers[-1](x), activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | test\n",
    "\n",
    "# test MultiActMLP\n",
    "key = jax.random.PRNGKey(0)\n",
    "model_key, init_key = jax.random.split(key)\n",
    "x = jnp.ones((5, 4))\n",
    "model = MultiActMLP(4, 2, [18], model_key, bias=False)\n",
    "model = init_linear_weight(model, deterministic_init, init_key)\n",
    "afuncs = [lambda x: x]\n",
    "y, _ = jax.vmap(model, in_axes=(0, None))(x, afuncs)\n",
    "# test 0 : see if model initializes correctly\n",
    "test_eq(jnp.all(model.layers[0].weight == 1e-3), True)\n",
    "\n",
    "# test 1 : see if activations work at all\n",
    "afuncs = [lambda x: 0]\n",
    "y, _ = jax.vmap(model, in_axes=(0, None))(x, afuncs)\n",
    "test_eq(jnp.all(y == 0), True)\n",
    "\n",
    "# test 2 : see if mixing activations works\n",
    "afuncs = [lambda x: 1, lambda x: 2, lambda x: 3]\n",
    "weights = model.layers[-1].weight.T\n",
    "dummy = jnp.ones((5, 18))\n",
    "dummy = dummy.at[:, 6:].set(2)\n",
    "dummy = dummy.at[:, 12:].set(3)\n",
    "y, _ = jax.vmap(model, in_axes=(0, None))(x, afuncs)\n",
    "test_eq(jnp.all(y == dummy @ weights), True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def save(filename, hyperparams, model):\n",
    "    \"\"\"save model and hyperparameters to file\"\"\"\n",
    "    with open(filename, \"wb\") as f:\n",
    "        hyperparam_str = json.dumps(hyperparams)\n",
    "        f.write((hyperparam_str + \"\\n\").encode())\n",
    "        eqx.tree_serialise_leaves(f, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | test\n",
    "config_dict = {\n",
    "    \"test_train_split\": 0.8,\n",
    "    \"input_dim\": 2,\n",
    "    \"output_dim\": 1,\n",
    "    \"hidden_layer_sizes\": [18],\n",
    "    \"batch_size\": 64,\n",
    "    \"epochs\": 2,\n",
    "    \"lr\": 1e-3,\n",
    "    \"mu\": 0.9,\n",
    "    \"n_fns\": 2,\n",
    "    \"l2_reg\": 1e-1,\n",
    "    \"seed\": 42,\n",
    "}\n",
    "key = jax.random.PRNGKey(config_dict[\"seed\"])\n",
    "model_key, init_key = jax.random.split(key)\n",
    "model = eqx.nn.MLP(\n",
    "    in_size=config_dict[\"input_dim\"],\n",
    "    out_size=config_dict[\"output_dim\"],\n",
    "    width_size=config_dict[\"hidden_layer_sizes\"][0],\n",
    "    depth=1,\n",
    "    key=model_key,\n",
    "    use_bias=True,\n",
    ")\n",
    "\n",
    "save(\"../output/test_model.eqx\", config_dict, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def make_mlp(config_dict):\n",
    "    \"\"\"initialize MLP using hyperparameters from config_dict\"\"\"\n",
    "    key = jax.random.PRNGKey(config_dict[\"seed\"])\n",
    "    model_key, init_key = jax.random.split(key)\n",
    "    model = eqx.nn.MLP(\n",
    "        in_size=config_dict[\"input_dim\"],\n",
    "        out_size=config_dict[\"output_dim\"],\n",
    "        width_size=config_dict[\"hidden_layer_sizes\"][0],\n",
    "        depth=1,\n",
    "        key=model_key,\n",
    "        use_bias=True,\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def load(filename, make=make_mlp):\n",
    "    \"\"\"load model and hyperparameters from file\"\"\"\n",
    "    with open(filename, \"rb\") as f:\n",
    "        hyperparams = json.loads(f.readline().decode())\n",
    "        model = make(hyperparams)\n",
    "        return eqx.tree_deserialise_leaves(f, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def mlp_afunc(x, model, base_act):\n",
    "    \"\"\"\n",
    "    MLP that behaves like an activation function\n",
    "    \"\"\"\n",
    "    new_x = x.reshape(-1, 1)\n",
    "    out = jax.vmap(model)(new_x)\n",
    "    return base_act(x) + out.reshape(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | test\n",
    "# test mlp_afunc\n",
    "key = jax.random.PRNGKey(0)\n",
    "model_key, init_key = jax.random.split(key)\n",
    "x = jnp.ones((6))\n",
    "model = eqx.nn.MLP(\n",
    "    in_size=1, out_size=1, width_size=18, depth=1, key=model_key, use_bias=False\n",
    ")\n",
    "model = init_linear_weight(model, deterministic_init, init_key)\n",
    "act = mlp_afunc(x, model, jnp.sin)\n",
    "test_eq(act.shape, x.shape)\n",
    "test_eq(act, jnp.sin(x) + model(jnp.ones(1))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
