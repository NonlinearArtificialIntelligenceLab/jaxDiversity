{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: XLA_PYTHON_CLIENT_ALLOCATOR=platform\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%reload_ext autoreload\n",
    "%env XLA_PYTHON_CLIENT_ALLOCATOR=platform"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataloading\n",
    "\n",
    "> Dataloaders for the neural networks. Uses pytorch dataloaders and converts to jax format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp dataloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.test import test_eq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import jax.numpy as jnp\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dummy Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# create a dummy dataset for testing\n",
    "class DummyDataset(Dataset):\n",
    "    \"\"\"Dummy dataset for testing\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_samples=100,  # number of samples\n",
    "        n_features=5,  # number of features\n",
    "        n_targets=1,  # number of targets\n",
    "        seed=42,  # random seed\n",
    "    ):\n",
    "        self.n_samples = n_samples\n",
    "        self.n_features = n_features\n",
    "        self.n_targets = n_targets\n",
    "        self.seed = seed\n",
    "        self.x = jnp.array(np.random.rand(n_samples, n_features))\n",
    "        self.y = jnp.array(np.random.rand(n_samples, n_targets))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def numpy_collate(batch):\n",
    "    if isinstance(batch[0], np.ndarray):\n",
    "        return np.stack(batch)\n",
    "    elif isinstance(batch[0], (tuple, list)):\n",
    "        transposed = zip(*batch)\n",
    "        return [numpy_collate(samples) for samples in transposed]\n",
    "    else:\n",
    "        return np.array(batch)\n",
    "\n",
    "\n",
    "class NumpyLoader(DataLoader):\n",
    "    \"\"\"A dataloader that uses numpy_collate\n",
    "    to allow numpy arrays instead of torch tensors\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset,\n",
    "        batch_size=1,\n",
    "        shuffle=False,\n",
    "        sampler=None,\n",
    "        batch_sampler=None,\n",
    "        num_workers=0,\n",
    "        pin_memory=False,\n",
    "        drop_last=False,\n",
    "        timeout=0,\n",
    "        worker_init_fn=None,\n",
    "    ):\n",
    "        super(self.__class__, self).__init__(\n",
    "            dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle,\n",
    "            sampler=sampler,\n",
    "            batch_sampler=batch_sampler,\n",
    "            num_workers=num_workers,\n",
    "            collate_fn=numpy_collate,\n",
    "            pin_memory=pin_memory,\n",
    "            drop_last=drop_last,\n",
    "            timeout=timeout,\n",
    "            worker_init_fn=worker_init_fn,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 4) (32, 4)\n"
     ]
    }
   ],
   "source": [
    "#| test\n",
    "dummy_dataset = DummyDataset(n_samples=250, n_features=4, n_targets=4, seed=42)\n",
    "dataloader = NumpyLoader(dummy_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "for x, y in dataloader:\n",
    "    print(x.shape, y.shape)\n",
    "    break\n",
    "\n",
    "test_eq(x.shape, (32, 4))\n",
    "test_eq(y.shape, (32, 4))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real Pendulum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class RealPendulumDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        root_path: str,  # path to data\n",
    "        train_split: float,  # fraction of data to use for training\n",
    "        train: bool = True,  # whether to use train or test data\n",
    "    ):\n",
    "        self.root_path = root_path\n",
    "        self.headers = [\"time\", \"theta\", \"theta_dot\", \"theta_ddot\"]\n",
    "        data = np.loadtxt(root_path + \"extracted_angles.csv\", delimiter=\",\")\n",
    "\n",
    "        if train:\n",
    "            data = data[: int(train_split * len(data))]\n",
    "        else:\n",
    "            data = data[int(train_split * len(data)) :]\n",
    "\n",
    "        # separate x(theta, theta_dot) and y(theta_dot, theta_ddot)\n",
    "        self.x = data[:, 1:3]\n",
    "        self.y = data[:, 2:]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return jnp.array(self.x[idx]), jnp.array(self.y[idx])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Henon Heiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class HHDataset(Dataset):\n",
    "    def __init__(\n",
    "        self, root_path, train  # path to data  # whether to use train or test data\n",
    "    ):\n",
    "        if train:\n",
    "            self.path = root_path + \"train/train.csv\"\n",
    "        else:\n",
    "            self.path = root_path + \"valid/valid.csv\"\n",
    "\n",
    "        data = np.loadtxt(self.path, delimiter=\",\", skiprows=1)\n",
    "        \n",
    "        # make a dictionary with keys \"x\" and \"y\" with x being q1, q2, p1, p2 and y being dq1, dq2, dp1, dp2\n",
    "        data = {\"x\": data[:, :4], \"y\": data[:, 5:]}\n",
    "        self.x = data[\"x\"][::400]\n",
    "        self.y = data[\"y\"][::400]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return jnp.array(self.x[idx]), jnp.array(self.y[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Swinging Sticks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class SwingingSticksDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        root_path,  # path to data\n",
    "        train_split,  # fraction of data to use for training\n",
    "        train=True,  # whether to use train or test data\n",
    "    ):\n",
    "        self.root_path = root_path\n",
    "        self.headers = [\n",
    "            \"time\",\n",
    "            \"theta_a\",\n",
    "            \"theta_A\",\n",
    "            \"theta_a_dot\",\n",
    "            \"theta_A_dot\",\n",
    "            \"theta_a_ddot\",\n",
    "            \"theta_A_ddot\",\n",
    "        ]\n",
    "        data = np.loadtxt(root_path + \"SSData.txt\", delimiter=\"\\t\")\n",
    "\n",
    "        if train:\n",
    "            data = data[: int(train_split * len(data))]\n",
    "        else:\n",
    "            data = data[int(train_split * len(data)) :]\n",
    "\n",
    "        # separate x and y\n",
    "        self.x = data[:, 1:5]\n",
    "        self.y = data[:, 3:]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return jnp.array(self.x[idx]), jnp.array(self.y[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
