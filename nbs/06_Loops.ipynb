{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: XLA_PYTHON_CLIENT_ALLOCATOR=platform\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%reload_ext autoreload\n",
    "%env XLA_PYTHON_CLIENT_ALLOCATOR=platform"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loops\n",
    "> Inner and outer loops for metalearning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import equinox as eqx\n",
    "import optax\n",
    "\n",
    "import logging\n",
    "from tqdm.notebook import trange\n",
    "import pickle\n",
    "from functools import partial\n",
    "import json\n",
    "from dataclasses import asdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from jaxDiversity.dataloading import NumpyLoader, DummyDataset\n",
    "from jaxDiversity.utilclasses import InnerConfig, OuterConfig, InnerResults, OuterResults\n",
    "from jaxDiversity.mlp import mlp_afunc, MultiActMLP, init_linear_weight, xavier_normal_init, save\n",
    "from jaxDiversity.baseline import compute_loss as compute_loss_baseline\n",
    "from jaxDiversity.hnn import compute_loss as compute_loss_hnn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the logger\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@eqx.filter_jit\n",
    "def make_step(model, x, y, afuncs, optim, opt_state, compute_loss):\n",
    "    loss, grads = compute_loss(model, x, y, afuncs)\n",
    "    updates, opt_state = optim.update(grads, opt_state)\n",
    "    model = eqx.apply_updates(model, updates)\n",
    "    return loss, grads, model, opt_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def inner_opt(model, train_data, test_data, afuncs, opt, loss_fn, config, training=False, verbose=False):\n",
    "    \"\"\"\n",
    "    inner optimization loop\n",
    "    \"\"\"\n",
    "    train_loss = []\n",
    "    test_loss = []\n",
    "    grad_norm = []\n",
    "\n",
    "    opt_state = opt.init(model)\n",
    "    for epoch in range(config.epochs):\n",
    "        if training:\n",
    "            for x,y in train_data:\n",
    "                loss, grads, model, opt_state = make_step(model, x, y, afuncs, opt, opt_state, loss_fn)\n",
    "                train_loss.append(loss)\n",
    "                grad_norm_tree = jax.tree_map(lambda x: jnp.linalg.norm(x), grads)\n",
    "                grad_norm_scalar = jax.tree_util.tree_reduce(jnp.add, grad_norm_tree)\n",
    "                grad_norm.append(grad_norm_scalar)\n",
    "        \n",
    "        for x,y in test_data:\n",
    "            x = jax.lax.stop_gradient(x)\n",
    "            y = jax.lax.stop_gradient(y)\n",
    "            loss, _ = loss_fn(model, x, y, afuncs)\n",
    "            test_loss.append(loss)\n",
    "        if verbose:\n",
    "            logging.info(f\"Epoch {epoch :03d} | Train Loss: {train_loss[-1] :.4e} | Test Loss: {test_loss[-1]:.4e} | Grad Norm: {grad_norm[-1]:.4e}\")\n",
    "\n",
    "    return model, opt_state, InnerResults(jnp.array(train_loss), jnp.array(test_loss), jnp.array(grad_norm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:jax._src.xla_bridge:Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: \"rocm\". Available platform names are: CUDA Interpreter Host\n",
      "INFO:jax._src.xla_bridge:Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'\n",
      "INFO:root:Baseline NN inner loop test\n",
      "INFO:root:Epoch 000 | Train Loss: 1.6608e-01 | Test Loss: 1.7718e-01 | Grad Norm: 4.0744e-01\n",
      "INFO:root:Epoch 001 | Train Loss: 1.4460e-01 | Test Loss: 1.2461e-01 | Grad Norm: 1.6079e-01\n"
     ]
    }
   ],
   "source": [
    "#| test\n",
    "# test inner_opt\n",
    "dev_inner_config = InnerConfig(test_train_split=0.8,\n",
    "                            input_dim=2,\n",
    "                            output_dim=2,\n",
    "                            hidden_layer_sizes=[18],\n",
    "                            batch_size=64,\n",
    "                            epochs=2,\n",
    "                            lr=1e-3,\n",
    "                            mu=0.9,\n",
    "                            n_fns=2,\n",
    "                            l2_reg=1e-1,\n",
    "                            seed=42)\n",
    "key = jax.random.PRNGKey(dev_inner_config.seed)\n",
    "model_key, init_key = jax.random.split(key)\n",
    "afuncs = [lambda x: x**2, lambda x: x]\n",
    "train_dataset = DummyDataset(1000, dev_inner_config.input_dim, dev_inner_config.output_dim)\n",
    "test_dataset = DummyDataset(1000, dev_inner_config.input_dim, dev_inner_config.output_dim)\n",
    "train_dataloader = NumpyLoader(train_dataset, batch_size=dev_inner_config.batch_size, shuffle=True)\n",
    "test_dataloader = NumpyLoader(test_dataset, batch_size=dev_inner_config.batch_size, shuffle=True)\n",
    "\n",
    "opt = optax.rmsprop(learning_rate=dev_inner_config.lr, momentum=dev_inner_config.mu, decay=dev_inner_config.l2_reg)\n",
    "model = MultiActMLP(dev_inner_config.input_dim, dev_inner_config.output_dim, dev_inner_config.hidden_layer_sizes, model_key, bias=False)\n",
    "logging.info(\"Baseline NN inner loop test\")\n",
    "baselineNN, opt_state ,inner_results = inner_opt(model =model, \n",
    "                                            train_data =train_dataloader,\n",
    "                                            test_data = test_dataloader,\n",
    "                                            afuncs = afuncs, \n",
    "                                            opt = opt, \n",
    "                                            loss_fn=compute_loss_baseline,\n",
    "                                            config = dev_inner_config, training=True, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Hamiltonian NN inner loop test\n",
      "INFO:root:Epoch 000 | Train Loss: 9.8625e-02 | Test Loss: 8.2503e-02 | Grad Norm: 2.8775e-01\n",
      "INFO:root:Epoch 001 | Train Loss: 7.7401e-02 | Test Loss: 7.7187e-02 | Grad Norm: 1.0838e-01\n"
     ]
    }
   ],
   "source": [
    "#| test\n",
    "# test inner_opt\n",
    "dev_inner_config = InnerConfig(test_train_split=0.8,\n",
    "                            input_dim=2,\n",
    "                            output_dim=1,\n",
    "                            hidden_layer_sizes=[18],\n",
    "                            batch_size=64,\n",
    "                            epochs=2,\n",
    "                            lr=1e-3,\n",
    "                            mu=0.9,\n",
    "                            n_fns=2,\n",
    "                            l2_reg=1e-1,\n",
    "                            seed=42)\n",
    "key = jax.random.PRNGKey(dev_inner_config.seed)\n",
    "model_key, init_key = jax.random.split(key)\n",
    "afuncs = [lambda x: x**2, lambda x: x]\n",
    "train_dataset = DummyDataset(1000, dev_inner_config.input_dim, 2)\n",
    "test_dataset = DummyDataset(1000, dev_inner_config.input_dim, 2)\n",
    "train_dataloader = NumpyLoader(train_dataset, batch_size=dev_inner_config.batch_size, shuffle=True)\n",
    "test_dataloader = NumpyLoader(test_dataset, batch_size=dev_inner_config.batch_size, shuffle=True)\n",
    "opt = optax.rmsprop(learning_rate=dev_inner_config.lr, momentum=dev_inner_config.mu, decay=dev_inner_config.l2_reg)\n",
    "model = MultiActMLP(dev_inner_config.input_dim, dev_inner_config.output_dim, dev_inner_config.hidden_layer_sizes, model_key, bias=False)\n",
    "\n",
    "logging.info(\"Hamiltonian NN inner loop test\")\n",
    "HNN, opt_state ,inner_results = inner_opt(model =model, \n",
    "                                            train_data =train_dataloader,\n",
    "                                            test_data = test_dataloader,\n",
    "                                            afuncs = afuncs, \n",
    "                                            opt = opt, \n",
    "                                            loss_fn=compute_loss_hnn,\n",
    "                                            config = dev_inner_config, training=True, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@eqx.filter_value_and_grad()\n",
    "def outer_loss(outer_models, inner_model, x, y, loss_fn, base_act):\n",
    "    inner_afuncs = [ partial(mlp_afunc, model = outer_model, base_act = base_act) for outer_model in outer_models]\n",
    "    loss, _ = loss_fn(inner_model, x, y, inner_afuncs)\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@eqx.filter_jit\n",
    "def outer_step(outer_models, inner_model, x, y, meta_opt, meta_opt_state, loss_fn, base_act):\n",
    "    loss, grads = outer_loss(outer_models, inner_model, x, y, loss_fn, base_act)\n",
    "    updates, opt_state = meta_opt.update(grads, meta_opt_state)\n",
    "    model = eqx.apply_updates(outer_models, updates)\n",
    "    return loss, grads, model, opt_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def outer_opt(train_dataloader, test_dataloader, loss_fn, inner_config, outer_config, opt, meta_opt, save_path=None):\n",
    "    \"\"\"\n",
    "    outer optimization loop\n",
    "    \"\"\"\n",
    "    outer_model_key = jax.random.PRNGKey(outer_config.seed)\n",
    "    inner_model_key = jax.random.PRNGKey(inner_config.seed)\n",
    "\n",
    "    outer_models = []\n",
    "    for _ in range(inner_config.n_fns):\n",
    "        model = eqx.nn.MLP(in_size=outer_config.input_dim,\n",
    "                        out_size=outer_config.output_dim,\n",
    "                        width_size=outer_config.hidden_layer_sizes[0],\n",
    "                        depth=1,\n",
    "                        key=outer_model_key,\n",
    "                        use_bias=True)\n",
    "        outer_models.append(model)\n",
    "\n",
    "    meta_opt_states = meta_opt.init(eqx.filter(outer_models, eqx.is_array))\n",
    "\n",
    "    results = {\n",
    "        \"train_loss\": [],\n",
    "        \"test_loss\": [],\n",
    "        \"inner_afuncs\": [],\n",
    "        \"grad_norms\": []\n",
    "    }\n",
    "\n",
    "    inner_afuncs = []\n",
    "\n",
    "    if inner_config.base_act == \"sin\":\n",
    "        base_act = jnp.sin\n",
    "    elif inner_config.base_act == \"relu\":\n",
    "        base_act = jnp.relu\n",
    "    elif inner_config.base_act == \"tanh\":\n",
    "        base_act = jnp.tanh\n",
    "\n",
    "    if save_path is not None:\n",
    "        # save config files\n",
    "        with open(f\"{save_path}/inner_config.json\", \"w\") as f:\n",
    "            json.dump(asdict(inner_config), f)\n",
    "        with open(f\"{save_path}/outer_config.json\", \"w\") as f:\n",
    "            json.dump(asdict(outer_config), f)\n",
    "\n",
    "\n",
    "    for step in trange(outer_config.steps):\n",
    "\n",
    "        inner_afuncs = [ partial(mlp_afunc, model = outer_model, base_act = base_act) for outer_model in outer_models]\n",
    "\n",
    "        inner_model = MultiActMLP(inner_config.input_dim,\n",
    "                                    inner_config.output_dim,\n",
    "                                    inner_config.hidden_layer_sizes,\n",
    "                                    inner_model_key, bias=True)\n",
    "        \n",
    "        inner_model = init_linear_weight(inner_model, xavier_normal_init, inner_model_key)\n",
    "\n",
    "        inner_model, _, inner_results = inner_opt(inner_model,\n",
    "                                    train_dataloader, test_dataloader,\n",
    "                                    inner_afuncs, opt, loss_fn,\n",
    "                                    inner_config, training=True,\n",
    "                                    verbose=False)\n",
    "\n",
    "        x,y = next(iter(train_dataloader))\n",
    "\n",
    "        \n",
    "        loss, grads, outer_models, meta_opt_states = outer_step(outer_models, inner_model, x, y, meta_opt, meta_opt_states, loss_fn, base_act)\n",
    "        grad_norm_tree = jax.tree_map(lambda x: jnp.linalg.norm(x), grads)\n",
    "        grad_norm_scalar = jax.tree_util.tree_reduce(jnp.add, grad_norm_tree)\n",
    "        results[\"grad_norms\"].append(grad_norm_scalar)\n",
    "        results[\"train_loss\"].append(loss)\n",
    "        mean_inner_test_loss = np.mean(inner_results.test_loss[-50:])\n",
    "        results[\"test_loss\"].append(mean_inner_test_loss)\n",
    "\n",
    "        if step % outer_config.print_every == 0:\n",
    "            logging.info(f\"Step {step :03d} | Train Loss: {results['train_loss'][-1] :.4e} | Test Loss: {mean_inner_test_loss :.4e} | Grad Norm: {results['grad_norms'][-1] :.4e}\")\n",
    "\n",
    "        # sample activation functions\n",
    "        x_sample = jnp.linspace(-10, 10, 100)\n",
    "        y_list = [x_sample]\n",
    "        for afunc in inner_afuncs:\n",
    "            y_list.append(afunc(x_sample))\n",
    "        results[\"inner_afuncs\"].append(y_list)\n",
    "\n",
    "\n",
    "        if (step % 100 == 0 or step == outer_config.steps-1) and save_path is not None:\n",
    "            # pickle and save results\n",
    "            with open(f\"{save_path}/step_{step}.pkl\", \"wb\") as f:\n",
    "                pickle.dump(results, f)\n",
    "            \n",
    "            # save models\n",
    "            for i, model in enumerate(outer_models):\n",
    "                save(f\"{save_path}/step_{step}_activation_model_{i}.eqx\", asdict(outer_config), model)\n",
    "    \n",
    "    results_obj = OuterResults(inner_test_loss= np.array(results[\"test_loss\"]),\n",
    "                               train_loss= np.array(results[\"train_loss\"]),\n",
    "                               inner_afuncs= np.array(results[\"inner_afuncs\"]),\n",
    "                                 grad_norm= np.array(results[\"grad_norms\"]))\n",
    "    \n",
    "    return outer_models, results_obj\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Baseline NN outer loop test\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61aa41ac3a4b48a69f061fe62e89dfcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Step 000 | Train Loss: 8.4353e-02 | Test Loss: 8.5589e-02 | Grad Norm: 6.3488e-01\n",
      "INFO:root:Step 001 | Train Loss: 9.1323e-02 | Test Loss: 8.8853e-02 | Grad Norm: 2.6783e-01\n"
     ]
    }
   ],
   "source": [
    "#| test\n",
    "# test outer_opt Baseline\n",
    "inner_config = InnerConfig(test_train_split=0.8,\n",
    "                            input_dim=2,\n",
    "                            output_dim=2,\n",
    "                            hidden_layer_sizes=[32],\n",
    "                            batch_size=64,\n",
    "                            epochs=5,\n",
    "                            lr=1e-3,\n",
    "                            mu=0.9,\n",
    "                            n_fns=2,\n",
    "                            l2_reg=1e-1,\n",
    "                            seed=42)\n",
    "outer_config = OuterConfig(input_dim=1,\n",
    "                            output_dim=1,\n",
    "                            hidden_layer_sizes=[18],\n",
    "                            batch_size=1,\n",
    "                            steps=2,\n",
    "                            print_every=1,\n",
    "                            lr=1e-3,\n",
    "                            mu=0.9,\n",
    "                            seed=24)\n",
    "train_dataset = DummyDataset(1000, inner_config.input_dim, inner_config.output_dim)\n",
    "test_dataset = DummyDataset(1000, inner_config.input_dim, inner_config.output_dim)\n",
    "train_dataloader = NumpyLoader(train_dataset, batch_size=inner_config.batch_size, shuffle=True)\n",
    "test_dataloader = NumpyLoader(test_dataset, batch_size=inner_config.batch_size, shuffle=True)\n",
    "\n",
    "opt = optax.rmsprop(learning_rate=inner_config.lr, momentum=inner_config.mu, decay=inner_config.l2_reg)\n",
    "meta_opt = optax.rmsprop(learning_rate=outer_config.lr, momentum=outer_config.mu)\n",
    "\n",
    "logging.info(\"Baseline NN outer loop test\")\n",
    "baseline_acts, baseline_stats = outer_opt(train_dataloader, test_dataloader,compute_loss_baseline ,inner_config, outer_config, opt, meta_opt, save_path=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Hamiltonian NN outer loop test\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2381e3d758345d58ad4293276418daf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Step 000 | Train Loss: 1.1194e-01 | Test Loss: 9.4442e-02 | Grad Norm: 8.8605e-01\n",
      "INFO:root:Step 001 | Train Loss: 8.8917e-02 | Test Loss: 9.3278e-02 | Grad Norm: 7.9053e-01\n"
     ]
    }
   ],
   "source": [
    "#| test\n",
    "# test outer_opt HNN\n",
    "inner_config = InnerConfig(test_train_split=0.8,\n",
    "                            input_dim=2,\n",
    "                            output_dim=1,\n",
    "                            hidden_layer_sizes=[32],\n",
    "                            batch_size=64,\n",
    "                            epochs=5,\n",
    "                            lr=1e-3,\n",
    "                            mu=0.9,\n",
    "                            n_fns=2,\n",
    "                            l2_reg=1e-1,\n",
    "                            seed=42)\n",
    "outer_config = OuterConfig(input_dim=1,\n",
    "                            output_dim=1,\n",
    "                            hidden_layer_sizes=[18],\n",
    "                            batch_size=1,\n",
    "                            steps=2,\n",
    "                            print_every=1,\n",
    "                            lr=1e-3,\n",
    "                            mu=0.9,\n",
    "                            seed=24)\n",
    "train_dataset = DummyDataset(1000, inner_config.input_dim, 2)\n",
    "test_dataset = DummyDataset(1000, inner_config.input_dim, 2)\n",
    "train_dataloader = NumpyLoader(train_dataset, batch_size=inner_config.batch_size, shuffle=True)\n",
    "test_dataloader = NumpyLoader(test_dataset, batch_size=inner_config.batch_size, shuffle=True)\n",
    "\n",
    "opt = optax.rmsprop(learning_rate=inner_config.lr, momentum=inner_config.mu, decay=inner_config.l2_reg)\n",
    "meta_opt = optax.rmsprop(learning_rate=outer_config.lr, momentum=outer_config.mu)\n",
    "\n",
    "logging.info(\"Hamiltonian NN outer loop test\")\n",
    "HNN_acts, HNN_stats = outer_opt(train_dataloader, test_dataloader,compute_loss_hnn ,inner_config, outer_config, opt, meta_opt, save_path=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
