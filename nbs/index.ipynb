{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# jaxDiversity\n",
    "\n",
    "> jax implementation for metalearning neuronal diversity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an updated implementation for Neural networks embrace diversity paper\n",
    "\n",
    "## Authors\n",
    "Anshul Choudhary, Anil Radhakrishnan, John F. Lindner, Sudeshna Sinha, and William L. Ditto\n",
    "\n",
    "## Link to paper\n",
    "* [arXiv](https://arxiv.org/abs/2204.04348)\n",
    "\n",
    "## Key Results\n",
    "* We construct neural networks with learnable activation functions and sere that they quickly diversify from each other under training. \n",
    "* These activations subsequently outperform their _pure_ counterparts on classification tasks.\n",
    "* The neuronal sub-networks instantiate the neurons and meta-learning adjusts their weights and biases to find efficient spanning sets of nonlinear activations.\n",
    "* These improved neural networks provide quantitative examples of the emergence of diversity and insight into its advantages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sh\n",
    "pip install jaxDiversity\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The codebase has 4 main components:\n",
    "* dataloading: Contains tools for loading the datasets mentioned in the manuscript. We use pytorch dataloaders with a custom numpy collate function to use this data in jax.\n",
    "\n",
    "* losses: We handle both traditional mlps and hamiltonian neural networkss with minimal changes with our loss implementations.\n",
    "\n",
    "* mlp: Contains custom mlp that takes in multiple activations and uses them _intralayer_ to create a diverse network. Also contains the activation neural networks.\n",
    "\n",
    "* loops: Contains the inner and outer loops for metalearning to optimize the activation functions in tandem with the supervised learning task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimum example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import optax\n",
    "\n",
    "from jaxDiversity.utilclasses import InnerConfig, OuterConfig # simple utility classes for configuration consistency\n",
    "from jaxDiversity.dataloading import NumpyLoader, DummyDataset\n",
    "from jaxDiversity.mlp import mlp_afunc, MultiActMLP, init_linear_weight, xavier_normal_init, save\n",
    "from jaxDiversity.baseline import compute_loss as compute_loss_baseline\n",
    "from jaxDiversity.hnn import compute_loss as compute_loss_hnn\n",
    "from jaxDiversity.loops import inner_opt, outer_opt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### inner optimzation or standard training loop with the baseline activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_inner_config = InnerConfig(test_train_split=0.8,\n",
    "                            input_dim=2,\n",
    "                            output_dim=2,\n",
    "                            hidden_layer_sizes=[18],\n",
    "                            batch_size=64,\n",
    "                            epochs=2,\n",
    "                            lr=1e-3,\n",
    "                            mu=0.9,\n",
    "                            n_fns=2,\n",
    "                            l2_reg=1e-1,\n",
    "                            seed=42)\n",
    "key = jax.random.PRNGKey(dev_inner_config.seed)\n",
    "model_key, init_key = jax.random.split(key)\n",
    "afuncs = [lambda x: x**2, lambda x: x]\n",
    "train_dataset = DummyDataset(1000, dev_inner_config.input_dim, dev_inner_config.output_dim)\n",
    "test_dataset = DummyDataset(1000, dev_inner_config.input_dim, dev_inner_config.output_dim)\n",
    "train_dataloader = NumpyLoader(train_dataset, batch_size=dev_inner_config.batch_size, shuffle=True)\n",
    "test_dataloader = NumpyLoader(test_dataset, batch_size=dev_inner_config.batch_size, shuffle=True)\n",
    "\n",
    "opt = optax.rmsprop(learning_rate=dev_inner_config.lr, momentum=dev_inner_config.mu, decay=dev_inner_config.l2_reg)\n",
    "model = MultiActMLP(dev_inner_config.input_dim, dev_inner_config.output_dim, dev_inner_config.hidden_layer_sizes, model_key, bias=False)\n",
    "baselineNN, opt_state ,inner_results = inner_opt(model =model, \n",
    "                                            train_data =train_dataloader,\n",
    "                                            test_data = test_dataloader,\n",
    "                                            afuncs = afuncs, \n",
    "                                            opt = opt, \n",
    "                                            loss_fn=compute_loss_baseline,\n",
    "                                            config = dev_inner_config, training=True, verbose=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### metalearning with Hamiltonian Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inner_config = InnerConfig(test_train_split=0.8,\n",
    "                            input_dim=2,\n",
    "                            output_dim=1,\n",
    "                            hidden_layer_sizes=[32],\n",
    "                            batch_size=64,\n",
    "                            epochs=5,\n",
    "                            lr=1e-3,\n",
    "                            mu=0.9,\n",
    "                            n_fns=2,\n",
    "                            l2_reg=1e-1,\n",
    "                            seed=42)\n",
    "outer_config = OuterConfig(input_dim=1,\n",
    "                            output_dim=1,\n",
    "                            hidden_layer_sizes=[18],\n",
    "                            batch_size=1,\n",
    "                            steps=2,\n",
    "                            print_every=1,\n",
    "                            lr=1e-3,\n",
    "                            mu=0.9,\n",
    "                            seed=24)\n",
    "train_dataset = DummyDataset(1000, inner_config.input_dim, 2)\n",
    "test_dataset = DummyDataset(1000, inner_config.input_dim, 2)\n",
    "train_dataloader = NumpyLoader(train_dataset, batch_size=inner_config.batch_size, shuffle=True)\n",
    "test_dataloader = NumpyLoader(test_dataset, batch_size=inner_config.batch_size, shuffle=True)\n",
    "\n",
    "opt = optax.rmsprop(learning_rate=inner_config.lr, momentum=inner_config.mu, decay=inner_config.l2_reg)\n",
    "meta_opt = optax.rmsprop(learning_rate=outer_config.lr, momentum=outer_config.mu)\n",
    "\n",
    "HNN_acts, HNN_stats = outer_opt(train_dataloader, test_dataloader,compute_loss_hnn ,inner_config, outer_config, opt, meta_opt, save_path=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Link to older pytorch codebase with classification problem: [DiversityNN](https://github.com/NonlinearArtificialIntelligenceLab/DiversityNN)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
