[
  {
    "objectID": "dataloading.html",
    "href": "dataloading.html",
    "title": "dataloading",
    "section": "",
    "text": "env: XLA_PYTHON_CLIENT_ALLOCATOR=platform"
  },
  {
    "objectID": "dataloading.html#dummy-dataset",
    "href": "dataloading.html#dummy-dataset",
    "title": "dataloading",
    "section": "Dummy Dataset",
    "text": "Dummy Dataset\n\nsource\n\nDummyDataset\n\n DummyDataset (n_samples=100, n_features=5, n_targets=1, seed=42)\n\nDummy dataset for testing\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nn_samples\nint\n100\nnumber of samples\n\n\nn_features\nint\n5\nnumber of features\n\n\nn_targets\nint\n1\nnumber of targets\n\n\nseed\nint\n42\nrandom seed\n\n\n\n\nsource\n\n\nNumpyLoader\n\n NumpyLoader (dataset, batch_size=1, shuffle=False, sampler=None,\n              batch_sampler=None, num_workers=0, pin_memory=False,\n              drop_last=False, timeout=0, worker_init_fn=None)\n\nA dataloader that uses numpy_collate to allow numpy arrays instead of torch tensors\n\nsource\n\n\nnumpy_collate\n\n numpy_collate (batch)\n\n\ndummy_dataset = DummyDataset(n_samples=250, n_features=4, n_targets=4, seed=42)\ndataloader = NumpyLoader(dummy_dataset, batch_size=32, shuffle=True)\n\nfor x, y in dataloader:\n    print(x.shape, y.shape)\n    break\n\ntest_eq(x.shape, (32, 4))\ntest_eq(y.shape, (32, 4))\n\n(32, 4) (32, 4)"
  },
  {
    "objectID": "dataloading.html#real-pendulum",
    "href": "dataloading.html#real-pendulum",
    "title": "dataloading",
    "section": "Real Pendulum",
    "text": "Real Pendulum\n\nsource\n\nRealPendulumDataset\n\n RealPendulumDataset (root_path:str, train_split:float, train:bool=True)\n\nAn abstract class representing a :class:Dataset.\nAll datasets that represent a map from keys to data samples should subclass it. All subclasses should overwrite :meth:__getitem__, supporting fetching a data sample for a given key. Subclasses could also optionally overwrite :meth:__len__, which is expected to return the size of the dataset by many :class:~torch.utils.data.Sampler implementations and the default options of :class:~torch.utils.data.DataLoader.\n.. note:: :class:~torch.utils.data.DataLoader by default constructs a index sampler that yields integral indices. To make it work with a map-style dataset with non-integral indices/keys, a custom sampler must be provided.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nroot_path\nstr\n\npath to data\n\n\ntrain_split\nfloat\n\nfraction of data to use for training\n\n\ntrain\nbool\nTrue\nwhether to use train or test data"
  },
  {
    "objectID": "dataloading.html#henon-heiles",
    "href": "dataloading.html#henon-heiles",
    "title": "dataloading",
    "section": "Henon Heiles",
    "text": "Henon Heiles\n\nsource\n\nHHDataset\n\n HHDataset (root_path, train)\n\nAn abstract class representing a :class:Dataset.\nAll datasets that represent a map from keys to data samples should subclass it. All subclasses should overwrite :meth:__getitem__, supporting fetching a data sample for a given key. Subclasses could also optionally overwrite :meth:__len__, which is expected to return the size of the dataset by many :class:~torch.utils.data.Sampler implementations and the default options of :class:~torch.utils.data.DataLoader.\n.. note:: :class:~torch.utils.data.DataLoader by default constructs a index sampler that yields integral indices. To make it work with a map-style dataset with non-integral indices/keys, a custom sampler must be provided.\n\n\n\n\nDetails\n\n\n\n\nroot_path\n\n\n\ntrain\npath to data # whether to use train or test data"
  },
  {
    "objectID": "dataloading.html#swinging-sticks",
    "href": "dataloading.html#swinging-sticks",
    "title": "dataloading",
    "section": "Swinging Sticks",
    "text": "Swinging Sticks\n\nsource\n\nSwingingSticksDataset\n\n SwingingSticksDataset (root_path, train_split, train=True)\n\nAn abstract class representing a :class:Dataset.\nAll datasets that represent a map from keys to data samples should subclass it. All subclasses should overwrite :meth:__getitem__, supporting fetching a data sample for a given key. Subclasses could also optionally overwrite :meth:__len__, which is expected to return the size of the dataset by many :class:~torch.utils.data.Sampler implementations and the default options of :class:~torch.utils.data.DataLoader.\n.. note:: :class:~torch.utils.data.DataLoader by default constructs a index sampler that yields integral indices. To make it work with a map-style dataset with non-integral indices/keys, a custom sampler must be provided.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nroot_path\n\n\npath to data\n\n\ntrain_split\n\n\nfraction of data to use for training\n\n\ntrain\nbool\nTrue\nwhether to use train or test data"
  },
  {
    "objectID": "loops.html",
    "href": "loops.html",
    "title": "Loops",
    "section": "",
    "text": "env: XLA_PYTHON_CLIENT_ALLOCATOR=platform\n\n\n\n# Configure the logger\nlogging.basicConfig(level=logging.INFO)\n\n\nsource\n\nmake_step\n\n make_step (model, x, y, afuncs, optim, opt_state, compute_loss)\n\n\nsource\n\n\ninner_opt\n\n inner_opt (model, train_data, test_data, afuncs, opt, loss_fn, config,\n            training=False, verbose=False)\n\ninner optimization loop\n\n# test inner_opt\ndev_inner_config = InnerConfig(\n    test_train_split=0.8,\n    input_dim=2,\n    output_dim=2,\n    hidden_layer_sizes=[18],\n    batch_size=64,\n    epochs=2,\n    lr=1e-3,\n    mu=0.9,\n    n_fns=2,\n    l2_reg=1e-1,\n    seed=42,\n)\nkey = jax.random.PRNGKey(dev_inner_config.seed)\nmodel_key, init_key = jax.random.split(key)\nafuncs = [lambda x: x**2, lambda x: x]\ntrain_dataset = DummyDataset(\n    1000, dev_inner_config.input_dim, dev_inner_config.output_dim\n)\ntest_dataset = DummyDataset(\n    1000, dev_inner_config.input_dim, dev_inner_config.output_dim\n)\ntrain_dataloader = NumpyLoader(\n    train_dataset, batch_size=dev_inner_config.batch_size, shuffle=True\n)\ntest_dataloader = NumpyLoader(\n    test_dataset, batch_size=dev_inner_config.batch_size, shuffle=True\n)\n\nopt = optax.rmsprop(\n    learning_rate=dev_inner_config.lr,\n    momentum=dev_inner_config.mu,\n    decay=dev_inner_config.l2_reg,\n)\nmodel = MultiActMLP(\n    dev_inner_config.input_dim,\n    dev_inner_config.output_dim,\n    dev_inner_config.hidden_layer_sizes,\n    model_key,\n    bias=False,\n)\nlogging.info(\"Baseline NN inner loop test\")\nbaselineNN, opt_state, inner_results = inner_opt(\n    model=model,\n    train_data=train_dataloader,\n    test_data=test_dataloader,\n    afuncs=afuncs,\n    opt=opt,\n    loss_fn=compute_loss_baseline,\n    config=dev_inner_config,\n    training=True,\n    verbose=True,\n)\n\nINFO:jax._src.xla_bridge:Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: \"rocm\". Available platform names are: CUDA Interpreter Host\nINFO:jax._src.xla_bridge:Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'\nINFO:root:Baseline NN inner loop test\nINFO:root:Epoch 000 | Train Loss: 1.6608e-01 | Test Loss: 1.7718e-01 | Grad Norm: 4.0744e-01\nINFO:root:Epoch 001 | Train Loss: 1.4460e-01 | Test Loss: 1.2461e-01 | Grad Norm: 1.6079e-01\n\n\n\n# test inner_opt\ndev_inner_config = InnerConfig(\n    test_train_split=0.8,\n    input_dim=2,\n    output_dim=1,\n    hidden_layer_sizes=[18],\n    batch_size=64,\n    epochs=2,\n    lr=1e-3,\n    mu=0.9,\n    n_fns=2,\n    l2_reg=1e-1,\n    seed=42,\n)\nkey = jax.random.PRNGKey(dev_inner_config.seed)\nmodel_key, init_key = jax.random.split(key)\nafuncs = [lambda x: x**2, lambda x: x]\ntrain_dataset = DummyDataset(1000, dev_inner_config.input_dim, 2)\ntest_dataset = DummyDataset(1000, dev_inner_config.input_dim, 2)\ntrain_dataloader = NumpyLoader(\n    train_dataset, batch_size=dev_inner_config.batch_size, shuffle=True\n)\ntest_dataloader = NumpyLoader(\n    test_dataset, batch_size=dev_inner_config.batch_size, shuffle=True\n)\nopt = optax.rmsprop(\n    learning_rate=dev_inner_config.lr,\n    momentum=dev_inner_config.mu,\n    decay=dev_inner_config.l2_reg,\n)\nmodel = MultiActMLP(\n    dev_inner_config.input_dim,\n    dev_inner_config.output_dim,\n    dev_inner_config.hidden_layer_sizes,\n    model_key,\n    bias=False,\n)\n\nlogging.info(\"Hamiltonian NN inner loop test\")\nHNN, opt_state, inner_results = inner_opt(\n    model=model,\n    train_data=train_dataloader,\n    test_data=test_dataloader,\n    afuncs=afuncs,\n    opt=opt,\n    loss_fn=compute_loss_hnn,\n    config=dev_inner_config,\n    training=True,\n    verbose=True,\n)\n\nINFO:root:Hamiltonian NN inner loop test\nINFO:root:Epoch 000 | Train Loss: 9.8625e-02 | Test Loss: 8.2503e-02 | Grad Norm: 2.8775e-01\nINFO:root:Epoch 001 | Train Loss: 7.7401e-02 | Test Loss: 7.7187e-02 | Grad Norm: 1.0838e-01\n\n\n\nsource\n\n\nouter_loss\n\n outer_loss (outer_models, inner_model, x, y, loss_fn, base_act)\n\n\nsource\n\n\nouter_step\n\n outer_step (outer_models, inner_model, x, y, meta_opt, meta_opt_state,\n             loss_fn, base_act)\n\n\nsource\n\n\nouter_opt\n\n outer_opt (train_dataloader, test_dataloader, loss_fn, inner_config,\n            outer_config, opt, meta_opt, save_path=None)\n\nouter optimization loop\n\n# test outer_opt Baseline\ninner_config = InnerConfig(\n    test_train_split=0.8,\n    input_dim=2,\n    output_dim=2,\n    hidden_layer_sizes=[32],\n    batch_size=64,\n    epochs=5,\n    lr=1e-3,\n    mu=0.9,\n    n_fns=2,\n    l2_reg=1e-1,\n    seed=42,\n)\nouter_config = OuterConfig(\n    input_dim=1,\n    output_dim=1,\n    hidden_layer_sizes=[18],\n    batch_size=1,\n    steps=2,\n    print_every=1,\n    lr=1e-3,\n    mu=0.9,\n    seed=24,\n)\ntrain_dataset = DummyDataset(1000, inner_config.input_dim, inner_config.output_dim)\ntest_dataset = DummyDataset(1000, inner_config.input_dim, inner_config.output_dim)\ntrain_dataloader = NumpyLoader(\n    train_dataset, batch_size=inner_config.batch_size, shuffle=True\n)\ntest_dataloader = NumpyLoader(\n    test_dataset, batch_size=inner_config.batch_size, shuffle=True\n)\n\nopt = optax.rmsprop(\n    learning_rate=inner_config.lr, momentum=inner_config.mu, decay=inner_config.l2_reg\n)\nmeta_opt = optax.rmsprop(learning_rate=outer_config.lr, momentum=outer_config.mu)\n\nlogging.info(\"Baseline NN outer loop test\")\nbaseline_acts, baseline_stats = outer_opt(\n    train_dataloader,\n    test_dataloader,\n    compute_loss_baseline,\n    inner_config,\n    outer_config,\n    opt,\n    meta_opt,\n    save_path=None,\n)\n\nINFO:root:Baseline NN outer loop test\nINFO:root:Step 000 | Train Loss: 8.4353e-02 | Test Loss: 8.5589e-02 | Grad Norm: 6.3488e-01\nINFO:root:Step 001 | Train Loss: 9.1323e-02 | Test Loss: 8.8853e-02 | Grad Norm: 2.6783e-01\n\n\n\n\n\n\n# test outer_opt HNN\ninner_config = InnerConfig(\n    test_train_split=0.8,\n    input_dim=2,\n    output_dim=1,\n    hidden_layer_sizes=[32],\n    batch_size=64,\n    epochs=5,\n    lr=1e-3,\n    mu=0.9,\n    n_fns=2,\n    l2_reg=1e-1,\n    seed=42,\n)\nouter_config = OuterConfig(\n    input_dim=1,\n    output_dim=1,\n    hidden_layer_sizes=[18],\n    batch_size=1,\n    steps=2,\n    print_every=1,\n    lr=1e-3,\n    mu=0.9,\n    seed=24,\n)\ntrain_dataset = DummyDataset(1000, inner_config.input_dim, 2)\ntest_dataset = DummyDataset(1000, inner_config.input_dim, 2)\ntrain_dataloader = NumpyLoader(\n    train_dataset, batch_size=inner_config.batch_size, shuffle=True\n)\ntest_dataloader = NumpyLoader(\n    test_dataset, batch_size=inner_config.batch_size, shuffle=True\n)\n\nopt = optax.rmsprop(\n    learning_rate=inner_config.lr, momentum=inner_config.mu, decay=inner_config.l2_reg\n)\nmeta_opt = optax.rmsprop(learning_rate=outer_config.lr, momentum=outer_config.mu)\n\nlogging.info(\"Hamiltonian NN outer loop test\")\nHNN_acts, HNN_stats = outer_opt(\n    train_dataloader,\n    test_dataloader,\n    compute_loss_hnn,\n    inner_config,\n    outer_config,\n    opt,\n    meta_opt,\n    save_path=None,\n)\n\nINFO:root:Hamiltonian NN outer loop test\nINFO:root:Step 000 | Train Loss: 1.1194e-01 | Test Loss: 9.4442e-02 | Grad Norm: 8.8605e-01\nINFO:root:Step 001 | Train Loss: 8.8917e-02 | Test Loss: 9.3278e-02 | Grad Norm: 7.9053e-01"
  },
  {
    "objectID": "baseline.html",
    "href": "baseline.html",
    "title": "baseline mlp",
    "section": "",
    "text": "env: XLA_PYTHON_CLIENT_ALLOCATOR=platform\n\n\n\nfrom jaxDiversity.mlp import MultiActMLP, deterministic_init, init_linear_weight\n\n\nsource\n\ncompute_loss\n\n compute_loss (model, x, y, afuncs)\n\nCompute the l2 loss of the model on the given data.\n\n# test compute_loss\nkey = jax.random.PRNGKey(0)\nmodel_key, init_key = jax.random.split(key)\nx = jnp.ones((5, 4))\n\nmodel = MultiActMLP(4, 2, [18], model_key, bias=False)\nmodel = init_linear_weight(model, deterministic_init, init_key)\ny = jnp.ones((5, 2))\n\nafuncs = [lambda x: 0, lambda x: 0]\n\nloss, grad = compute_loss(model, x, y, afuncs)\ntest_eq(loss, 1)"
  },
  {
    "objectID": "mlp.html",
    "href": "mlp.html",
    "title": "MLP",
    "section": "",
    "text": "env: XLA_PYTHON_CLIENT_ALLOCATOR=platform\n\n\n\nsource\n\nxavier_uniform_init\n\n xavier_uniform_init (weight:jax.Array, key:&lt;function PRNGKey&gt;)\n\nxavier uniform initialization\n\nsource\n\n\nxavier_normal_init\n\n xavier_normal_init (weight:jax.Array, key:&lt;function PRNGKey&gt;)\n\nxavier normal initialization\n\nsource\n\n\ndeterministic_init\n\n deterministic_init (weight:jax.Array, key:&lt;function PRNGKey&gt;)\n\nconstant initialization parameters only for consistency with other initializations\n\nsource\n\n\ntrunc_init\n\n trunc_init (weight:jax.Array, key:&lt;function PRNGKey&gt;)\n\ntruncated normal initialization\n\nsource\n\n\ninit_linear_weight\n\n init_linear_weight (model, init_fn, key)\n\ninitialize linear weights of a model with a given init_fn\n\nsource\n\n\nMultiActMLP\n\n MultiActMLP (*args, **kwargs)\n\n\n# test MultiActMLP\nkey = jax.random.PRNGKey(0)\nmodel_key, init_key = jax.random.split(key)\nx = jnp.ones((5, 4))\nmodel = MultiActMLP(4, 2, [18], model_key, bias=False)\nmodel = init_linear_weight(model, deterministic_init, init_key)\nafuncs = [lambda x: x]\ny, _ = jax.vmap(model, in_axes=(0, None))(x, afuncs)\n# test 0 : see if model initializes correctly\ntest_eq(jnp.all(model.layers[0].weight == 1e-3), True)\n\n# test 1 : see if activations work at all\nafuncs = [lambda x: 0]\ny, _ = jax.vmap(model, in_axes=(0, None))(x, afuncs)\ntest_eq(jnp.all(y == 0), True)\n\n# test 2 : see if mixing activations works\nafuncs = [lambda x: 1, lambda x: 2, lambda x: 3]\nweights = model.layers[-1].weight.T\ndummy = jnp.ones((5, 18))\ndummy = dummy.at[:, 6:].set(2)\ndummy = dummy.at[:, 12:].set(3)\ny, _ = jax.vmap(model, in_axes=(0, None))(x, afuncs)\ntest_eq(jnp.all(y == dummy @ weights), True)\n\n\nsource\n\n\nsave\n\n save (filename, hyperparams, model)\n\nsave model and hyperparameters to file\n\nsource\n\n\nmake_mlp\n\n make_mlp (config_dict)\n\ninitialize MLP using hyperparameters from config_dict\n\nsource\n\n\nload\n\n load (filename, make=&lt;function make_mlp&gt;)\n\nload model and hyperparameters from file\n\nsource\n\n\nmlp_afunc\n\n mlp_afunc (x, model, base_act)\n\nMLP that behaves like an activation function\n\n# test mlp_afunc\nkey = jax.random.PRNGKey(0)\nmodel_key, init_key = jax.random.split(key)\nx = jnp.ones((6))\nmodel = eqx.nn.MLP(\n    in_size=1, out_size=1, width_size=18, depth=1, key=model_key, use_bias=False\n)\nmodel = init_linear_weight(model, deterministic_init, init_key)\nact = mlp_afunc(x, model, jnp.sin)\ntest_eq(act.shape, x.shape)\ntest_eq(act, jnp.sin(x) + model(jnp.ones(1))[0])"
  },
  {
    "objectID": "utilclasses.html",
    "href": "utilclasses.html",
    "title": "Util classes",
    "section": "",
    "text": "source\n\nInnerConfig\n\n InnerConfig (test_train_split:float, input_dim:int, output_dim:int,\n              hidden_layer_sizes:list, batch_size:int, epochs:int,\n              lr:float, mu:float, n_fns:int, l2_reg:float, seed:int,\n              base_act:str='sin')\n\n\nsource\n\n\nOuterConfig\n\n OuterConfig (input_dim:int, output_dim:int, hidden_layer_sizes:list,\n              batch_size:int, steps:int, print_every:int, lr:float,\n              mu:float, seed:int)\n\n\nsource\n\n\nInnerResults\n\n InnerResults (train_loss:list, test_loss:list, grad_norm:list)\n\ndataclass to store inner loop results\n\nsource\n\n\nOuterResults\n\n OuterResults (inner_test_loss:list, train_loss:list, inner_afuncs:list,\n               grad_norm:list)\n\ndataclass to store outer loop results"
  },
  {
    "objectID": "hnn.html",
    "href": "hnn.html",
    "title": "Hamiltonian Neural Network",
    "section": "",
    "text": "env: XLA_PYTHON_CLIENT_ALLOCATOR=platform\n\n\n\nsource\n\nhamiltonian_factory\n\n hamiltonian_factory (model, afuncs)\n\nReturns a function that computes the Hamiltonian of a given model.\n\nsource\n\n\ncompute_loss\n\n compute_loss (model, x, y, afuncs)\n\nComputes hamiltonâ€™s equations to get dqdp and then computes the loss\n\n# test compute_loss\nkey = jax.random.PRNGKey(0)\nmodel_key, init_key = jax.random.split(key)\nx = jnp.ones((5, 2))\n\nmodel = MultiActMLP(2, 1, [18], model_key, bias=False)\nmodel = init_linear_weight(model, deterministic_init, init_key)\ny = jnp.ones((5, 2))\n\nafuncs = [lambda x: 1, lambda x: 0]\n\nloss, _ = compute_loss(model, x, y, afuncs)\ntest_eq(loss, 1.0)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "jaxDiversity",
    "section": "",
    "text": "This is an updated implementation for Neural networks embrace diversity paper"
  },
  {
    "objectID": "index.html#authors",
    "href": "index.html#authors",
    "title": "jaxDiversity",
    "section": "Authors",
    "text": "Authors\nAnshul Choudhary, Anil Radhakrishnan, John F. Lindner, Sudeshna Sinha, and William L. Ditto"
  },
  {
    "objectID": "index.html#link-to-paper",
    "href": "index.html#link-to-paper",
    "title": "jaxDiversity",
    "section": "Link to paper",
    "text": "Link to paper\n\narXiv"
  },
  {
    "objectID": "index.html#key-results",
    "href": "index.html#key-results",
    "title": "jaxDiversity",
    "section": "Key Results",
    "text": "Key Results\n\nWe construct neural networks with learnable activation functions and sere that they quickly diversify from each other under training.\nThese activations subsequently outperform their pure counterparts on classification tasks.\nThe neuronal sub-networks instantiate the neurons and meta-learning adjusts their weights and biases to find efficient spanning sets of nonlinear activations.\nThese improved neural networks provide quantitative examples of the emergence of diversity and insight into its advantages."
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "jaxDiversity",
    "section": "Install",
    "text": "Install\npip install jaxDiversity"
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "jaxDiversity",
    "section": "How to use",
    "text": "How to use\nThe codebase has 4 main components: * dataloading: Contains tools for loading the datasets mentioned in the manuscript. We use pytorch dataloaders with a custom numpy collate function to use this data in jax.\n\nlosses: We handle both traditional mlps and hamiltonian neural networkss with minimal changes with our loss implementations.\nmlp: Contains custom mlp that takes in multiple activations and uses them intralayer to create a diverse network. Also contains the activation neural networks.\nloops: Contains the inner and outer loops for metalearning to optimize the activation functions in tandem with the supervised learning task\n\n\nMinimum example\n\nimport jax\nimport optax\n\nfrom jaxDiversity.utilclasses import (\n    InnerConfig,\n    OuterConfig,\n)  # simple utility classes for configuration consistency\nfrom jaxDiversity.dataloading import NumpyLoader, DummyDataset\nfrom jaxDiversity.mlp import (\n    mlp_afunc,\n    MultiActMLP,\n    init_linear_weight,\n    xavier_normal_init,\n    save,\n)\nfrom jaxDiversity.baseline import compute_loss as compute_loss_baseline\nfrom jaxDiversity.hnn import compute_loss as compute_loss_hnn\nfrom jaxDiversity.loops import inner_opt, outer_opt\n\n\ninner optimzation or standard training loop with the baseline activation\n\ndev_inner_config = InnerConfig(\n    test_train_split=0.8,\n    input_dim=2,\n    output_dim=2,\n    hidden_layer_sizes=[18],\n    batch_size=64,\n    epochs=2,\n    lr=1e-3,\n    mu=0.9,\n    n_fns=2,\n    l2_reg=1e-1,\n    seed=42,\n)\nkey = jax.random.PRNGKey(dev_inner_config.seed)\nmodel_key, init_key = jax.random.split(key)\nafuncs = [lambda x: x**2, lambda x: x]\ntrain_dataset = DummyDataset(\n    1000, dev_inner_config.input_dim, dev_inner_config.output_dim\n)\ntest_dataset = DummyDataset(\n    1000, dev_inner_config.input_dim, dev_inner_config.output_dim\n)\ntrain_dataloader = NumpyLoader(\n    train_dataset, batch_size=dev_inner_config.batch_size, shuffle=True\n)\ntest_dataloader = NumpyLoader(\n    test_dataset, batch_size=dev_inner_config.batch_size, shuffle=True\n)\n\nopt = optax.rmsprop(\n    learning_rate=dev_inner_config.lr,\n    momentum=dev_inner_config.mu,\n    decay=dev_inner_config.l2_reg,\n)\nmodel = MultiActMLP(\n    dev_inner_config.input_dim,\n    dev_inner_config.output_dim,\n    dev_inner_config.hidden_layer_sizes,\n    model_key,\n    bias=False,\n)\nbaselineNN, opt_state, inner_results = inner_opt(\n    model=model,\n    train_data=train_dataloader,\n    test_data=test_dataloader,\n    afuncs=afuncs,\n    opt=opt,\n    loss_fn=compute_loss_baseline,\n    config=dev_inner_config,\n    training=True,\n    verbose=True,\n)\n\n\n\nmetalearning with Hamiltonian Neural Networks\n\ninner_config = InnerConfig(\n    test_train_split=0.8,\n    input_dim=2,\n    output_dim=1,\n    hidden_layer_sizes=[32],\n    batch_size=64,\n    epochs=5,\n    lr=1e-3,\n    mu=0.9,\n    n_fns=2,\n    l2_reg=1e-1,\n    seed=42,\n)\nouter_config = OuterConfig(\n    input_dim=1,\n    output_dim=1,\n    hidden_layer_sizes=[18],\n    batch_size=1,\n    steps=2,\n    print_every=1,\n    lr=1e-3,\n    mu=0.9,\n    seed=24,\n)\ntrain_dataset = DummyDataset(1000, inner_config.input_dim, 2)\ntest_dataset = DummyDataset(1000, inner_config.input_dim, 2)\ntrain_dataloader = NumpyLoader(\n    train_dataset, batch_size=inner_config.batch_size, shuffle=True\n)\ntest_dataloader = NumpyLoader(\n    test_dataset, batch_size=inner_config.batch_size, shuffle=True\n)\n\nopt = optax.rmsprop(\n    learning_rate=inner_config.lr, momentum=inner_config.mu, decay=inner_config.l2_reg\n)\nmeta_opt = optax.rmsprop(learning_rate=outer_config.lr, momentum=outer_config.mu)\n\nHNN_acts, HNN_stats = outer_opt(\n    train_dataloader,\n    test_dataloader,\n    compute_loss_hnn,\n    inner_config,\n    outer_config,\n    opt,\n    meta_opt,\n    save_path=None,\n)\n\nLink to older pytorch codebase with classification problem: DiversityNN"
  }
]